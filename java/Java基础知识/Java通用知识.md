# Java通用知识

数据在计算机中实际上都是**二进制形式表示的**，但是人难以阅读编程。于是，**高级语言引入了数据类型、变量的概念**——用来区分不同的数据，通过编译器来将数据转换成对应的二进制表示

程序的流程控制有两种：1. 条件执行；2. 循环

## （一）数的二进制表示

### 1. 整数的二进制表示

正整数：通过原码进行表示，且正整数的**原码 = 补码**

负整数：通过补码进行表示：流程就是：只取其整数部分，忽略前面的负号，然后**将其原码表示出来，然后全部取反，+1**

eg： -1 => 0000 0001 => 1111 1110 => 1111 1111; -127 => 0111 1111 => 1000 0000 => 1000 0001

所以，可以直接通过最高位1/0，判断出该数是正数还是负数

那么。如果一个数是有符号的，那么其表示的数的绝对值会变小，即：8位二进制，如果是表示无符号数，那么能表示0~255；如果是有符号数，那么能表示-127~0~128（因为**最高位是符号位**）

用补码表示的原因：**计算机中减法本质上就是加法，+相反数**，eg：5 - 3=> 5 + (-3)

<img src="C:/Users/surface/Desktop/计算机知识整理/java/pic/image-20201108194515909.png" alt="image-20201108194515909"  />

所以能理解加法上溢会变成负数：

<img src="C:/Users/surface/Desktop/计算机知识整理/java/pic/image-20201108194619414.png" alt="image-20201108194619414"  />

### 2. 小数的二进制表示

因为计算机是按照二进制计算的，所以在小数方面有天然缺陷，即使在一些非常基本的小数运算中，计算的结果也是不精确的。

eg：

```java
float x1 = 0.1f * 0.1f;
>> x1 = 0.010000001
```

二进制格式不能精确表示0.1，它只能表示一个非常接近0.1但又不等于0.1的一个数

对于小数：2的某次方之和（某次方，是指负数）的数可以精确表示，其他数则不能精确表示

计算不精确的方法：

- 如果不需要那么高的精度，可以四舍五入，或者在输出的时候只保留固定个数的小数位；
- 如果需要，将小数转化为整数进行运算，运算结束后再转化为小数；或者转换成十进制再计算，比较复杂

二进制中表示小数，用的是科学计数法：`m× (2^e)`（m称为尾数，e称为指数），其实，还有一个符号位表示正负

几乎所有的硬件和编程语言表示小数的二进制格式都是一样的，有2种格式：

- 32位的：1位符号 + 23位尾数 + 8位指数
- 64位的：1位符号 + 52位尾数 + 11位指数

除了表示正常的数，标准还规定了一些特殊的二进制形式表示一些特殊的值，比如**负无穷、正无穷、0、NaN**（非数值，比如0乘以无穷大）

## 3. 字符编码

### 1. 非Unicode编码

有很多非Unicode编码，常见的有：**ASCII**、ISO 8859-1、Windows-1252、**GB2312**、**GBK**、GB18030和Big5

西欧国家中流行的是ISO 8859-1和Windows-1252；中国是GB2312、GBK、GB18030和Big5

- ASCII：美国信息互换标准代码，7位，128个字符，在一个字节中，统一将最高位设置为0
- ISO 8859-1、Windows-1252：128~255，即最高位为1，和ASCII兼容，当最高位为0时，代表使用ASCII；当最高位为1时，代表使用自己的码
- GB2312：包括约7000个汉字和一些罕用词和繁体字，**两个字节表示汉字**，同时和ASCII兼容，方法同上，最高位设置为0.
- GBK：建立在GB2312的基础上，向下兼容GB2312，约21 000个汉字，其中包括繁体字。**两个字节**

——其他编码都是兼容ASCII的，最高位使用0/1来进行区分

### 2. Unicode编码

Unicode将所有编码进行统一，给世界上所有字符都分配了一个唯一的数字编号

**Unicode本身只规定了每个字符的数字编号是多少**，但是没有指定对应的二进制表示

有多种方案，主要有UTF-32、UTF-16和UTF-8。

- UTF-32：统一用4字节，浪费空间
- UTF-16：用变长字节表示，至少2个
- UTF-8：使用的字节个数为1～4不等。

只有UTF-8兼容ASCII

### 3. 编码转换

编码转换的具体过程可以是：一个字符从A编码转到B编码，

- 先找到字符的A编码格式，
- 通过A的映射表找到其Unicode编号，
- 然后通过Unicode编号再查B的映射表，找到字符的B编码格式。

### 4. 乱码

1. 解析错误——错误识别

   解析数据的方式错了，换成正确的解析方式即可，即本来是Windows-1252类型的，看成了GB2312，那么就将其当成GB2312的二进制来解析，就错误了。

2. 先解析错误，后进行编码转换了，此时就回不去了——错误转换

   即本来是Windows-1252类型的，看成了GB2312，并且还将其转换成了UTF-8形式，然后转换成GB18030，然后就是乱码了

恢复的方法：尝试进行逆向操作，将原始的数据正确看成其对应的类型，然后点击编码转换，指定正确的转换方向